{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment analysist .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlyvKTrJfEx0yxGpCG2HCj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annisann/sentiment-analysis-for-conversational-texts/blob/dev/sentiment_analysist_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4J1ER2dlAAK"
      },
      "source": [
        "**Script Utilities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTbVc6jvlCTq"
      },
      "source": [
        "#@title Git Credential { display-mode: \"both\" }\n",
        "#@markdown Exec this cell first\n",
        "username = \"bayooow\" #@param {type:\"string\"}\n",
        "password = \"Az10291234\" #@param {type:\"string\"}\n",
        "name = \"Bayu Aji Firmansyah\" #@param {type:\"string\"}\n",
        "email = \"shoesproject98@gmail.com\" #@param {type:\"string\"}\n",
        "\n",
        "# init project\n",
        "! git config --global user.name \"{name}\"\n",
        "! git config --global user.username \"{username}\"\n",
        "! git config --global user.email \"{email}\"\n",
        "! git clone https://github.com/annisann/sentiment-analysis-for-conversational-texts.git && cd sentiment-analysis-for-conversational-texts \n",
        "! cd sentiment-analysis-for-conversational-texts && git remote rm origin \n",
        "! cd sentiment-analysis-for-conversational-texts && git remote add origin https://{username}:{password}@github.com/annisann/sentiment-analysis-for-conversational-texts.git\n",
        "! cd sentiment-analysis-for-conversational-texts && git fetch && git checkout dev\n",
        "# install dependency\n",
        "! pip install Sastrawi\n",
        "! pip install deprecated\n",
        "\n",
        "# work directory\n",
        "basePath = \"/content/sentiment-analysis-for-conversational-texts/\"\n",
        "datasetPath = basePath + \"dataset/\"\n",
        "rawPath = datasetPath + \"raw/\"\n",
        "processedPath = datasetPath + \"processed/\"\n",
        "trainPath = datasetPath + \"train/\"\n",
        "testPath = datasetPath + \"test/\"\n",
        "modelPath = basePath +\"model/\"\n",
        "tfidfPath = modelPath + \"tfidf/\"\n",
        "classifierPath = modelPath + \"classifier/\"\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "R7mU3lfnlsvR"
      },
      "source": [
        "#@title Commit the change { vertical-output: true }\n",
        "#@markdown Exec this cell to push change to repository\n",
        "# commit change to dev\n",
        "\n",
        "CommitMessage = \"add example result\" #@param {type:\"string\"}\n",
        "\n",
        "! cd /content/sentiment-analysis-for-conversational-texts && git add . && git commit -m \"{CommitMessage}\" && git push "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1552KcbJDUK"
      },
      "source": [
        "**Project Structure** </br>\n",
        "- root \n",
        "  - dataset \n",
        "    - raw\n",
        "    - preprocessed\n",
        "    - train\n",
        "    - test\n",
        "  - model \n",
        "    - tfidf\n",
        "    - classifier\n",
        "      - run_x\n",
        "- src\n",
        "  - notebook \n",
        "  - main \n",
        "    - core\n",
        "    - utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AUuOlFZK_Sc"
      },
      "source": [
        "**utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4NbT57mK-r4"
      },
      "source": [
        "# work directory\n",
        "basePath = \"/content/sentiment-analysis-for-conversational-texts/\"\n",
        "datasetPath = basePath + \"dataset/\"\n",
        "rawPath = datasetPath + \"raw/\"\n",
        "processedPath = datasetPath + \"processed/\"\n",
        "trainPath = datasetPath + \"train/\"\n",
        "testPath = datasetPath + \"test/\"\n",
        "modelPath = basePath +\"model/\"\n",
        "tfidfPath = modelPath + \"tfidf/\"\n",
        "classifierPath = modelPath + \"classifier/\"\n",
        "\n",
        "def isSpecialTerm(term):\n",
        "  specialTerm = [\"[DOSEN]\", \"[MHS]\", \"[URL]\", \"[EMAIL]\", \"<DOC>\"]\n",
        "  return term in specialTerm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjN6Lkp_IjHC"
      },
      "source": [
        "**Preprocessing :**  </br>\n",
        "casefolding -> stemming -> stopword removal "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOcwxsTAJCo5"
      },
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from deprecated import deprecated\n",
        "import string \n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import json\n",
        "\n",
        "#global varible\n",
        "nltk.download('stopwords')\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()  \n",
        "idStopWord = set(stopwords.words('indonesian'))\n",
        "enStopWord = set(stopwords.words('english'))\n",
        "\n",
        "def casefold(rawText):\n",
        "  if isinstance(rawText , list):\n",
        "    return [str(text).lower() for text in rawText]\n",
        "  return str(rawText).lower()\n",
        "\n",
        "def stem(rawText, toArray = False):\n",
        "  if isinstance(rawText, list):\n",
        "    rawText = \" \".join(rawText)\n",
        "\n",
        "  if toArray:\n",
        "    return stemmer.stem(rawText).split(\" \")\n",
        "\n",
        "  return  stemmer.stem(rawText)\n",
        "\n",
        "def clean(rawText):\n",
        "\n",
        "  def _replaceURL(text):\n",
        "    urlRegex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
        "    return re.sub(urlRegex, \"[URL]\", text)\n",
        "\n",
        "  def _replaceEmail(text):\n",
        "    urlRegex = r'([a-zA-Z0-9\\.\\_\\-]+@+[a-zA-Z0-9.]+)'\n",
        "    return re.sub(urlRegex, \"[EMAIL]\", text)\n",
        "    \n",
        "\n",
        "  def _removePunctuation(text):\n",
        "    return re.sub(r'([!?.,;])\\1+', '', str(text))\n",
        "\n",
        "  def _normalizeWhitespace(text):\n",
        "    corrected = str(text)\n",
        "    corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n",
        "    corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n",
        "    corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n",
        "    corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n",
        "    corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n",
        "    return corrected.strip(\" \")\n",
        "\n",
        "  @deprecated\n",
        "  def _simplifyPunctuation(text):\n",
        "    return re.sub(r'([!?,;])\\1+', r'\\1', str(text))\n",
        "    \n",
        "  if isinstance(rawText , list):\n",
        "    rawText = \" \".join(rawText)\n",
        "\n",
        "  rawText = _replaceURL(rawText)\n",
        "  rawText = _replaceEmail(rawText)\n",
        "  rawText = _normalizeWhitespace(rawText)\n",
        "  rawText = _removePunctuation(rawText)\n",
        "\n",
        "  return rawText\n",
        "\n",
        "def filter(rawText):\n",
        "  if not isinstance(rawText, list):\n",
        "    rawText = rawText.split(\" \")\n",
        "  return [w for w in rawText if not w in idStopWord and w not in enStopWord]\n",
        "\n",
        "def normalize(rawText):\n",
        "  with open(datasetPath+'slang_words.json', 'r') as f:\n",
        "    dict = json.load(f)\n",
        "\n",
        "  if  not isinstance(rawText , list):\n",
        "      rawText = rawText.split(\" \")\n",
        "  token = []\n",
        "  for word in rawText:\n",
        "      slang_dict = {v:k for v, k in dict.items()}\n",
        "      normal = slang_dict.get(word, word)\n",
        "      token.append(normal)\n",
        "  return token\n",
        "\n",
        "def pipelinePreprocess(rawText):\n",
        "  rawText = casefold(rawText)\n",
        "  rawText = clean(rawText)\n",
        "  rawText = stem(rawText)\n",
        "  rawText = normalize(rawText)\n",
        "  rawText = filter(rawText)\n",
        "  return rawText\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsXgNR7DnYdS"
      },
      "source": [
        "**Indexing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AonjgZnnckt"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "\n",
        "files = (glob.glob(rawPath+\"*.csv\"))\n",
        "indexToFile = {i:file for (i, file) in enumerate(files)}\n",
        "fileToIndex = {file:i for (i, file) in enumerate(files)}\n",
        "\n",
        "try:\n",
        "  os.remove(processedPath+\"indexToFile.json\")\n",
        "except:\n",
        "  print()\n",
        "\n",
        "with open(processedPath+\"indexToFile.json\", \"w\") as outfile: \n",
        "  json.dump(indexToFile, outfile)\n",
        "\n",
        "try:\n",
        "    os.remove(processedPath+\"fileToIndex.json\")\n",
        "except:\n",
        "  print()\n",
        "  \n",
        "with open(processedPath+\"fileToIndex.json\", \"w\") as outfile: \n",
        "  json.dump(fileToIndex, outfile)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_5P0bR_rf6E"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x-s8egwnxBz"
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOpjnzXr8LLM"
      },
      "source": [
        "def sessionManagement(file):\n",
        "  df = pd.read_csv(file)\n",
        "  df = df[df.timestamp.notnull()]\n",
        "  sessions = df.timestamp.unique().tolist()\n",
        "  sessionToIndex = {file:i for (i, file) in enumerate(sessions)}\n",
        "  indexToSession = {i:file for (i, file) in enumerate(sessions)}\n",
        "\n",
        "  try:\n",
        "    os.remove(processedPath+str(fileToIndex[file])+\"/indexToSession.json\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  with open(processedPath+str(fileToIndex[file])+\"/indexToSession.json\", \"w\") as outfile: \n",
        "    json.dump(indexToSession, outfile)\n",
        "\n",
        "  try:\n",
        "    os.remove(processedPath+str(fileToIndex[file])+\"/sessionToIndex.json\")\n",
        "  except:\n",
        "    pass\n",
        "    \n",
        "  with open(processedPath+str(fileToIndex[file])+\"/sessionToIndex.json\", \"w\") as outfile: \n",
        "    json.dump(sessionToIndex, outfile)\n",
        "\n",
        "  df[\"session_index\"] = [sessionToIndex[session] for session in df.timestamp]\n",
        "  df[\"file_index\"] = [fileToIndex[file] for session in df.timestamp]\n",
        "  return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taVpHEaAsqtb"
      },
      "source": [
        "def preprocess(df):\n",
        "  messages = df.messages.tolist()\n",
        "  preprocessed = []\n",
        "  for message in messages:\n",
        "    resp = pipelinePreprocess(message)\n",
        "    preprocessed.append(resp)\n",
        "  df[\"preprocessed\"] = preprocessed\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5foiO6D8bk3"
      },
      "source": [
        "# preprocess every file\n",
        "files = (glob.glob(rawPath+\"*.csv\"))\n",
        "for file in files :\n",
        "  raw = sessionManagement(file)\n",
        "  preprocessed = preprocess(raw)\n",
        "  preprocessed.to_csv(workingDirectory+str(fileToIndex[file])+\"_preprocessed.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyE7dFIsAFAO"
      },
      "source": [
        "# compile all preprocessed file\n",
        "\n",
        "import fnmatch\n",
        "import os\n",
        "import pandas as pd \n",
        "\n",
        "matches = []\n",
        "for root, dirnames, filenames in os.walk(processedPath):\n",
        "    for filename in fnmatch.filter(filenames, '*.csv'):\n",
        "        matches.append(os.path.join(root, filename))\n",
        "\n",
        "counter = 1\n",
        "for match in matches:\n",
        "  printHeader = False\n",
        "  if counter ==1 : \n",
        "    printHeader = True\n",
        "\n",
        "  df = pd.read_csv(match)\n",
        "  df.to_csv(processedPath+\"compiled.csv\", header=printHeader, mode=\"a\", index=False)\n",
        "  counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG7Y02hPGnGG"
      },
      "source": [
        "**build data train and test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgtKj19sGtVU"
      },
      "source": [
        "#check distribusi kelas\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(processedPath+\"compiled.csv\")\n",
        "df['unique_index'] =(df.session_index.astype(str) +\"_\"+ df.file_index.astype(str))\n",
        "sessionDF = df.drop_duplicates(subset=['unique_index'], keep='last')\n",
        "positiveSession = sessionDF[sessionDF.sentiment_per_sesi == 1]\n",
        "negativeSession = sessionDF[sessionDF.sentiment_per_sesi == -1]\n",
        "\n",
        "\n",
        "plt.bar([\"Positive (\"+str(len(positiveSession))+\" Sessions)\", \"Negative (\"+str(len(negativeSession))+\" Sessions)\"], [len(positiveSession), len(negativeSession)])\n",
        "plt.title('Dataset Session Sentiment Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Number of Session ('+str(len(positiveSession) + len(negativeSession))+' sessions)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tHcnORuGq5O"
      },
      "source": [
        "import random \n",
        "\n",
        "trainPercentage = 80\n",
        "testPercentage = 20\n",
        "\n",
        "\n",
        "maximumNumberOfClass = min(len(positiveSession), len(negativeSession))\n",
        "\n",
        "nTrain = int(trainPercentage /100 * maximumNumberOfClass)\n",
        "nTest = int(testPercentage /100 * maximumNumberOfClass)\n",
        "\n",
        "positiveID = positiveSession.unique_index.tolist()\n",
        "negativeID = negativeSession.unique_index.tolist()\n",
        "trainID = random.sample(positiveID, nTrain) + random.sample(negativeID, nTrain)\n",
        "\n",
        "for id in trainID:\n",
        "  try:\n",
        "    positiveID.remove(id)\n",
        "  except:\n",
        "    negativeID.remove(id)\n",
        "\n",
        "testID = random.sample(positiveID, nTest)\n",
        "testID.append(random.sample(negativeID, nTest))\n",
        "\n",
        "df[df.unique_index.isin(trainID)].to_csv(trainPath+\"train.csv\", index = False)\n",
        "df[df.unique_index.isin(testID)].to_csv(testPath+\"test.csv\", index = False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFg8dJa1QMYW"
      },
      "source": [
        "**Building Model Base**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8Aa-JnYOUpu"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import string \n",
        "import ast\n",
        "import pickle\n",
        "\n",
        "def trainTFIDFModel(path=trainPath+\"train.csv\"):\n",
        "  df = pd.read_csv(path)\n",
        "  messages = [\" \".join(ast.literal_eval(message)) for message in df.preprocessed]\n",
        "  vectorizer = TfidfVectorizer(use_idf=True)\n",
        "  X = vectorizer.fit(messages)\n",
        "\n",
        "\n",
        "  with open(tfidfPath+\"model.pickle\", 'wb') as pickleFile:\n",
        "    pickle.dump(vectorizer, pickleFile)\n",
        "\n",
        "def loadTFIDFModel(path = tfidfPath+\"model.pickle\"):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  with open(path, 'rb') as pickleFile:\n",
        "    # print(pickle.load(pickleFile))\n",
        "    return pickle.load(pickleFile)\n",
        "  \n",
        "def TFIDF(terms, model):\n",
        "  return model.transform(terms)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyWGOymkfJP3"
      },
      "source": [
        "**Do TFIDF to Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR5EPbFERXXr"
      },
      "source": [
        "tfidfModel = loadTFIDFModel()\n",
        "#  convert data train\n",
        "df = pd.read_csv(trainPath+\"train.csv\")\n",
        "messages = [\" \".join(ast.literal_eval(message)) for message in df.preprocessed]\n",
        "vectorized = TFIDF(messages, tfidfModel)\n",
        "vectorized = pd.DataFrame(vectorized.todense())\n",
        "vectorized['unique_index'] = df.unique_index\n",
        "vectorized['sentiment_per_chat'] = df.sentiment_per_chat\n",
        "vectorized['sentiment_per_sesi'] = df.sentiment_per_sesi\n",
        "vectorized.to_csv(trainPath+\"train_feed.csv\")\n",
        "\n",
        "df = pd.read_csv(testPath+\"test.csv\")\n",
        "messages = [\" \".join(ast.literal_eval(message)) for message in df.preprocessed]\n",
        "vectorized = TFIDF(messages, tfidfModel)\n",
        "vectorized = pd.DataFrame(vectorized.todense())\n",
        "vectorized['unique_index'] = df.unique_index\n",
        "vectorized['sentiment_per_chat'] = df.sentiment_per_chat\n",
        "vectorized['sentiment_per_sesi'] = df.sentiment_per_sesi\n",
        "vectorized.to_csv(testPath+\"test_feed.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kaju2Mqhiet"
      },
      "source": [
        "**Base Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKOAWZxamjRL"
      },
      "source": [
        "def transformInput(path = trainPath+\"train_feed.csv\"):\n",
        "  target = []\n",
        "  feature = []\n",
        "  vectorized = pd.read_csv(path)\n",
        "\n",
        "  uniqueID = vectorized.unique_index.unique().tolist()\n",
        "  for unique in uniqueID:\n",
        "    d = vectorized[vectorized.unique_index == unique]\n",
        "    expected = [d.iloc[0].sentiment_per_sesi]\n",
        "    if expected == [-1]:\n",
        "      expected = [0] \n",
        "    target.append(expected)\n",
        "    d = d.drop('unique_index',1)\n",
        "    d = d.drop('sentiment_per_chat',1)\n",
        "    d = d.drop('sentiment_per_sesi',1)\n",
        "    d = d.drop(d.columns[0], axis=1)\n",
        "    feature.append(d.to_numpy())\n",
        "  \n",
        "  return np.array(feature), np.array(target)\n",
        "\n",
        "def dataGenerator(data, target, batch_size=1):\n",
        "  counter = 0 \n",
        "  samples_per_epoch = data.shape[0]\n",
        "  number_of_batches = samples_per_epoch/batch_size\n",
        "  while True:\n",
        "    \n",
        "    yield np.array([data[counter]]), target[counter]\n",
        "\n",
        "    counter += 1\n",
        "    if counter >= number_of_batches:\n",
        "        counter = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teAHVMJZXzSh"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import pickle\n",
        "import os \n",
        "import glob\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import json \n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "def train(epochs = 10, batchSize = 1, verbose = 1, path = trainPath+\"train_feed.csv\"):\n",
        "  dataTrain, groundTruth = transformInput(path)\n",
        "  history  = model.fit_generator(dataGenerator(dataTrain, groundTruth, batchSize),  steps_per_epoch=dataTrain.shape[0]/batchSize, epochs=epochs, verbose=verbose)\n",
        "  return history\n",
        "\n",
        "def evaluate(batchSize = 10, path = testPath+\"test_feed.csv\"):\n",
        "  dataTrain, groundTruth = transformInput(path)\n",
        "  result = model.evaluate_generator(dataGenerator(dataTrain, groundTruth, batchSize),  steps=dataTrain.shape[0]/batchSize, verbose=0)\n",
        "  return result\n",
        "\n",
        "def predict(data):\n",
        "  return model.predict(data)\n",
        "\n",
        "def save(model, history, res):\n",
        "  def getLastIndex():\n",
        "    folder = [f for f in glob.iglob(classifierPath+\"run_*\")]\n",
        "    return len(folder)\n",
        "  processID = getLastIndex()+1\n",
        "  workingDir = classifierPath+\"run_\"+str(processID)+\"/\"\n",
        "  os.mkdir(workingDir)\n",
        "\n",
        "  plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "  metadata = {\n",
        "      \"process_id\":processID,\n",
        "      \"model_param\":history.params,\n",
        "      \"train_result\": history.history,\n",
        "      \"evaluation_result\":{\n",
        "          \"metric_name\":model.metrics_names,\n",
        "          \"value\":res\n",
        "      }\n",
        "  }\n",
        "  with open(workingDir+\"summary.json\", \"w\") as fw : \n",
        "    json.dump(metadata, fw, indent=4)\n",
        "\n",
        "  with open(workingDir+\"model_configuration.json\", \"w\") as fw : \n",
        "    json.dump(model.get_config(), fw)\n",
        "\n",
        "  with open(workingDir+'architecture.txt','w+') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model.summary()\n",
        "\n",
        "  model.save_weights(workingDir+\"model_weight.h5\")\n",
        "\n",
        "  plot_model(model, to_file= workingDir+'model.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "\n",
        "def loadModel(processID):\n",
        "  path = classifierPath+\"run_\"+str(processID)\n",
        "  with open(path+\"/model_configuration.json\", 'r') as json_file:\n",
        "    json_file = json.load(json_file)\n",
        "  model = tf.keras.Sequential.from_config((json_file))\n",
        "  model.load_weights(path + \"/model_weight.h5\")\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_izUlyg2dsTm"
      },
      "source": [
        "**Tunning di marih**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EuwafCJVDMb"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Input( (None, 1262)),\n",
        "  tf.keras.layers.LSTM(1262,activation='tanh', recurrent_activation='sigmoid',use_bias=True, return_sequences=True),\n",
        "  tf.keras.layers.LSTM(1262,activation='tanh', recurrent_activation='sigmoid',use_bias=True),\n",
        "  tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "model.build((None, None,1262))\n",
        "\n",
        "history = train(batchSize=10, epochs=10 , verbose=1)\n",
        "resp = evaluate()\n",
        "save(model, history, resp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrqvdCesdxZa"
      },
      "source": [
        "**Visualisasi History**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "civ4CAVEnDvR"
      },
      "source": [
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "summaryPaths = [p for p in Path(classifierPath).rglob('summary.json')]\n",
        "\n",
        "for summaryPath in summaryPaths:\n",
        "  with open(summaryPath,\"r\") as f:\n",
        "    summary = (json.load(f))\n",
        "    print(\"===========================xxxxxxxxxxx===============================\")\n",
        "    print(\"process id: \",summary[\"process_id\"])\n",
        "    print(\"parameter : \",summary[\"model_param\"])\n",
        "\n",
        "    summaryPath = str(summaryPath).split(\"/\")[:-1]\n",
        "    summaryPath = \"/\".join(summaryPath)+\"/architecture.txt\"\n",
        "    print(open(summaryPath, \"r\").read())\n",
        "\n",
        "    print(\"evaluation loss\" , summary[\"evaluation_result\"][\"value\"][0])\n",
        "    print(\"evaluation accuracy \",summary[\"evaluation_result\"][\"value\"][1])\n",
        "    print()\n",
        "    plt.plot(summary[\"train_result\"][\"loss\"])\n",
        "    plt.plot(summary[\"train_result\"][\"accuracy\"])\n",
        "    plt.title('model training accuracy loss')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['loss', 'accuracy'], loc='upper left')\n",
        "    plt.show()\n",
        "    print()\n",
        "    print()\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQZReXBtZqYv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SVrnb31mEb4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}