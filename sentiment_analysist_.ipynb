{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/annisann/sentiment-analysis-for-conversational-texts/blob/dev/sentiment_analysist_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Script Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1552KcbJDUK"
   },
   "source": [
    "**Project Structure** </br>\n",
    "- root \n",
    "  - dataset \n",
    "    - raw\n",
    "    - preprocessed\n",
    "    - train\n",
    "    - test\n",
    "  - model \n",
    "    - tfidf\n",
    "    - classifier\n",
    "      - run_x\n",
    "- src\n",
    "  - notebook \n",
    "  - main \n",
    "    - core\n",
    "    - utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AUuOlFZK_Sc"
   },
   "source": [
    "**utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # work directory\n",
    "# basePath = \"/content/sentiment-analysis-for-conversational-texts/\"\n",
    "# datasetPath = basePath + \"dataset/\"\n",
    "# rawPath = datasetPath + \"raw/\"\n",
    "# processedPath = datasetPath + \"processed/\"\n",
    "# trainPath = datasetPath + \"train/\"\n",
    "# testPath = datasetPath + \"test/\"\n",
    "# modelPath = basePath +\"model/\"\n",
    "# tfidfPath = modelPath + \"tfidf/\"\n",
    "# classifierPath = modelPath + \"classifier/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z4NbT57mK-r4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# work directory\n",
    "basePath = os.getcwd() + '/'\n",
    "datasetPath = basePath + \"dataset/\"\n",
    "rawPath = datasetPath + \"raw/\"\n",
    "processedPath = datasetPath + \"processed/\"\n",
    "trainPath = datasetPath + \"train/\"\n",
    "testPath = datasetPath + \"test/\"\n",
    "modelPath = basePath +\"model/\"\n",
    "tfidfPath = modelPath + \"tfidf/\"\n",
    "classifierPath = modelPath + \"classifier/\"\n",
    "\n",
    "def isSpecialTerm(term):\n",
    "    specialTerm = [\"[DOSEN]\", \"[MHS]\", \"[URL]\", \"[EMAIL]\", \"<DOC>\"]\n",
    "    return term in specialTerm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjN6Lkp_IjHC"
   },
   "source": [
    "**Preprocessing :**  </br>\n",
    "casefolding -> stemming -> stopword removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rOcwxsTAJCo5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from deprecated import deprecated\n",
    "import string \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "#global varible\n",
    "nltk.download('stopwords')\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()  \n",
    "idStopWord = set(stopwords.words('indonesian'))\n",
    "enStopWord = set(stopwords.words('english'))\n",
    "\n",
    "def casefold(rawText):\n",
    "    if isinstance(rawText , list):\n",
    "        return [str(text).lower() for text in rawText]\n",
    "    return str(rawText).lower()\n",
    "\n",
    "def stem(rawText, toArray = False):\n",
    "    if isinstance(rawText, list):\n",
    "        rawText = \" \".join(rawText)\n",
    "\n",
    "    if toArray:\n",
    "        return stemmer.stem(rawText).split(\" \")\n",
    "\n",
    "    return  stemmer.stem(rawText)\n",
    "\n",
    "def clean(rawText):\n",
    "   \n",
    "    def _replaceURL(text):\n",
    "        urlRegex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "        return re.sub(urlRegex, \"[URL]\", text)\n",
    "\n",
    "    def _replaceEmail(text):\n",
    "        urlRegex = r'([a-zA-Z0-9\\.\\_\\-]+@+[a-zA-Z0-9.]+)'\n",
    "        return re.sub(urlRegex, \"[EMAIL]\", text)\n",
    "    \n",
    "    def _removePunctuation(text):\n",
    "        return re.sub(r'([!?.,;])\\1+', '', str(text))\n",
    "\n",
    "    def _normalizeWhitespace(text):\n",
    "        corrected = str(text)\n",
    "        corrected = re.sub(r\"//t\",r\"\\t\", corrected)\n",
    "        corrected = re.sub(r\"( )\\1+\",r\"\\1\", corrected)\n",
    "        corrected = re.sub(r\"(\\n)\\1+\",r\"\\1\", corrected)\n",
    "        corrected = re.sub(r\"(\\r)\\1+\",r\"\\1\", corrected)\n",
    "        corrected = re.sub(r\"(\\t)\\1+\",r\"\\1\", corrected)\n",
    "        return corrected.strip(\" \")\n",
    "\n",
    "    @deprecated\n",
    "    def _simplifyPunctuation(text):\n",
    "        return re.sub(r'([!?,;])\\1+', r'\\1', str(text))\n",
    "    \n",
    "    if isinstance(rawText , list):\n",
    "        rawText = \" \".join(rawText)\n",
    "\n",
    "    rawText = _replaceURL(rawText)\n",
    "    rawText = _replaceEmail(rawText)\n",
    "    rawText = _normalizeWhitespace(rawText)\n",
    "    rawText = _removePunctuation(rawText)\n",
    "\n",
    "    return rawText\n",
    "\n",
    "def filter(rawText):\n",
    "    if not isinstance(rawText, list):\n",
    "        rawText = rawText.split(\" \")\n",
    "    return [w for w in rawText if not w in idStopWord and w not in enStopWord]\n",
    "\n",
    "def normalize(rawText):\n",
    "    with open(datasetPath+'slang_words.json', 'r') as f:\n",
    "        dict = json.load(f)\n",
    "\n",
    "    if  not isinstance(rawText , list):\n",
    "        rawText = rawText.split(\" \")\n",
    "    token = []\n",
    "    for word in rawText:\n",
    "        slang_dict = {v:k for v, k in dict.items()}\n",
    "        normal = slang_dict.get(word, word)\n",
    "        token.append(normal)\n",
    "    return token\n",
    "\n",
    "def pipelinePreprocess(rawText):\n",
    "    rawText = casefold(rawText)\n",
    "    rawText = clean(rawText)\n",
    "    rawText = stem(rawText)\n",
    "    rawText = normalize(rawText)\n",
    "    rawText = filter(rawText)\n",
    "    return rawText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsXgNR7DnYdS"
   },
   "source": [
    "**Indexing Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5AonjgZnnckt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "files = (glob.glob(rawPath+\"*.csv\"))\n",
    "indexToFile = {i:file for (i, file) in enumerate(files)}\n",
    "fileToIndex = {file:i for (i, file) in enumerate(files)}\n",
    "\n",
    "try:\n",
    "    os.remove(processedPath+\"indexToFile.json\")\n",
    "except:\n",
    "    print()\n",
    "\n",
    "with open(processedPath+\"indexToFile.json\", \"w\") as outfile: \n",
    "    json.dump(indexToFile, outfile)\n",
    "\n",
    "try:\n",
    "    os.remove(processedPath+\"fileToIndex.json\")\n",
    "except:\n",
    "    print()\n",
    "\n",
    "with open(processedPath+\"fileToIndex.json\", \"w\") as outfile: \n",
    "    json.dump(fileToIndex, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_5P0bR_rf6E"
   },
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_x-s8egwnxBz"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MOpjnzXr8LLM"
   },
   "outputs": [],
   "source": [
    "def sessionManagement(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df = df[df.timestamp.notnull()]\n",
    "    sessions = df.timestamp.unique().tolist()\n",
    "    sessionToIndex = {file:i for (i, file) in enumerate(sessions)}\n",
    "    indexToSession = {i:file for (i, file) in enumerate(sessions)}\n",
    "\n",
    "    try:\n",
    "        os.remove(processedPath+str(fileToIndex[file])+\"/indexToSession.json\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    with open(processedPath+str(fileToIndex[file])+\"/indexToSession.json\", \"w\") as outfile: \n",
    "        json.dump(indexToSession, outfile)\n",
    "\n",
    "    try:\n",
    "        os.remove(processedPath+str(fileToIndex[file])+\"/sessionToIndex.json\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    with open(processedPath+str(fileToIndex[file])+\"/sessionToIndex.json\", \"w\") as outfile: \n",
    "        json.dump(sessionToIndex, outfile)\n",
    "\n",
    "    df[\"session_index\"] = [sessionToIndex[session] for session in df.timestamp]\n",
    "    df[\"file_index\"] = [fileToIndex[file] for session in df.timestamp]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "taVpHEaAsqtb"
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    messages = df.messages.tolist()\n",
    "    preprocessed = []\n",
    "    for message in messages:\n",
    "        resp = pipelinePreprocess(message)\n",
    "        preprocessed.append(resp)\n",
    "    df[\"preprocessed\"] = preprocessed\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "E5foiO6D8bk3"
   },
   "outputs": [],
   "source": [
    "# preprocess every file\n",
    "files = (glob.glob(rawPath+\"*.csv\"))\n",
    "for file in files :\n",
    "    raw = sessionManagement(file)\n",
    "    preprocessed = preprocess(raw)\n",
    "    preprocessed.to_csv(processedPath+str(fileToIndex[file])+'/'+str(fileToIndex[file])+'_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fyE7dFIsAFAO"
   },
   "outputs": [],
   "source": [
    "# compile all preprocessed file\n",
    "\n",
    "import fnmatch\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "matches = []\n",
    "for root, dirnames, filenames in os.walk(processedPath):\n",
    "    for filename in fnmatch.filter(filenames, '*.csv'):\n",
    "        matches.append(os.path.join(root, filename))\n",
    "\n",
    "counter = 1\n",
    "for match in matches:\n",
    "    printHeader = False\n",
    "    if counter == 1:\n",
    "        printHeader = True\n",
    "\n",
    "    df = pd.read_csv(match)\n",
    "    df.to_csv(processedPath+\"compiled.csv\", header=printHeader, mode=\"a\", index=False)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG7Y02hPGnGG"
   },
   "source": [
    "**build data train and test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CgtKj19sGtVU"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check distribusi kelas\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(processedPath+\"compiled.csv\")\n",
    "df['unique_index'] =(df.session_index.astype(str) +\"_\"+ df.file_index.astype(str))\n",
    "sessionDF = df.drop_duplicates(subset=['unique_index'], keep='last')\n",
    "positiveSession = sessionDF[sessionDF.sentiment_per_sesi == 1]\n",
    "negativeSession = sessionDF[sessionDF.sentiment_per_sesi == -1]\n",
    "\n",
    "\n",
    "plt.bar([\"Positive (\"+str(len(positiveSession))+\" Sessions)\", \"Negative (\"+str(len(negativeSession))+\" Sessions)\"], [len(positiveSession), len(negativeSession)])\n",
    "plt.title('Dataset Session Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Session ('+str(len(positiveSession) + len(negativeSession))+' sessions)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0tHcnORuGq5O"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "trainPercentage = 80\n",
    "testPercentage = 20\n",
    "\n",
    "maximumNumberOfClass = min(len(positiveSession), len(negativeSession))\n",
    "\n",
    "nTrain = int(trainPercentage /100 * maximumNumberOfClass)\n",
    "nTest = int(testPercentage /100 * maximumNumberOfClass)\n",
    "\n",
    "positiveID = positiveSession.unique_index.tolist()\n",
    "negativeID = negativeSession.unique_index.tolist()\n",
    "trainID = random.sample(positiveID, nTrain) + random.sample(negativeID, nTrain)\n",
    "\n",
    "for id in trainID:\n",
    "    try:\n",
    "        positiveID.remove(id)\n",
    "    except:\n",
    "        negativeID.remove(id)\n",
    "\n",
    "testID = random.sample(positiveID, nTest)\n",
    "testID.append(random.sample(negativeID, nTest))\n",
    "\n",
    "df[df.unique_index.isin(trainID)].to_csv(trainPath+\"train.csv\", index = False)\n",
    "df[df.unique_index.isin(testID)].to_csv(testPath+\"test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFg8dJa1QMYW"
   },
   "source": [
    "**Building Model Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "M8Aa-JnYOUpu"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import string \n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "def trainTFIDFModel(path=trainPath+\"train.csv\"):\n",
    "    df = pd.read_csv(path)\n",
    "    messages = [\" \".join(ast.literal_eval(message)) for message in df.preprocessed]\n",
    "    vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    X = vectorizer.fit(messages)\n",
    "\n",
    "    with open(tfidfPath+\"model.pickle\", 'wb') as pickleFile:\n",
    "        pickle.dump(vectorizer, pickleFile)\n",
    "\n",
    "def loadTFIDFModel(path = tfidfPath+\"model.pickle\"):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    with open(path, 'rb') as pickleFile:\n",
    "#         print(pickle.load(pickleFile))\n",
    "        return pickle.load(pickleFile)\n",
    "  \n",
    "def TFIDF(terms, model):\n",
    "    return model.transform(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyWGOymkfJP3"
   },
   "source": [
    "**Do TFIDF to Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KR5EPbFERXXr"
   },
   "outputs": [],
   "source": [
    "# tfidfModel = trainTFIDFModel()\n",
    "tfidfModel = loadTFIDFModel()\n",
    "#  convert data train\n",
    "df = pd.read_csv(trainPath+\"train.csv\")\n",
    "messages = [\" \".join(ast.literal_eval(message)) for message in df.preprocessed]\n",
    "vectorized = TFIDF(messages, tfidfModel)\n",
    "vectorized = pd.DataFrame(vectorized.todense())\n",
    "vectorized['unique_index'] = df.unique_index\n",
    "vectorized['sentiment_per_chat'] = df.sentiment_per_chat\n",
    "vectorized['sentiment_per_sesi'] = df.sentiment_per_sesi\n",
    "vectorized.to_csv(trainPath+\"train_feed.csv\")\n",
    "\n",
    "df = pd.read_csv(testPath+\"test.csv\")\n",
    "messages = [\" \".join(ast.literal_eval(message)) for message in df.preprocessed]\n",
    "vectorized = TFIDF(messages, tfidfModel)\n",
    "vectorized = pd.DataFrame(vectorized.todense())\n",
    "vectorized['unique_index'] = df.unique_index\n",
    "vectorized['sentiment_per_chat'] = df.sentiment_per_chat\n",
    "vectorized['sentiment_per_sesi'] = df.sentiment_per_sesi\n",
    "vectorized.to_csv(testPath+\"test_feed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Kaju2Mqhiet"
   },
   "source": [
    "**Base Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RKOAWZxamjRL"
   },
   "outputs": [],
   "source": [
    "def transformInput(path = trainPath+\"train_feed.csv\"):\n",
    "    target = []\n",
    "    feature = []\n",
    "    vectorized = pd.read_csv(path)\n",
    "\n",
    "    uniqueID = vectorized.unique_index.unique().tolist()\n",
    "    for unique in uniqueID:\n",
    "        d = vectorized[vectorized.unique_index == unique]\n",
    "        expected = [d.iloc[0].sentiment_per_sesi]\n",
    "        if expected == [-1]:\n",
    "            expected = [0] \n",
    "    target.append(expected)\n",
    "    d = d.drop('unique_index',1)\n",
    "    d = d.drop('sentiment_per_chat',1)\n",
    "    d = d.drop('sentiment_per_sesi',1)\n",
    "    d = d.drop(d.columns[0], axis=1)\n",
    "    feature.append(d.to_numpy())\n",
    "  \n",
    "    return np.array(feature), np.array(target)\n",
    "\n",
    "def dataGenerator(data, target, batch_size=1):\n",
    "    counter = 0 \n",
    "    samples_per_epoch = data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    while True:\n",
    "        yield np.array([data[counter]]), target[counter]\n",
    "\n",
    "        counter += 1\n",
    "        if counter >= number_of_batches:\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "teAHVMJZXzSh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'bincount_ops' from 'tensorflow.python.ops' (D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-fe6521707d0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbincount_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbitwise_ops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcond_v2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'bincount_ops' from 'tensorflow.python.ops' (D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pickle\n",
    "import os \n",
    "import glob\n",
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import json \n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "def train(epochs = 10, batchSize = 1, verbose = 1, path = trainPath+\"train_feed.csv\"):\n",
    "    dataTrain, groundTruth = transformInput(path)\n",
    "    history  = model.fit_generator(dataGenerator(dataTrain, groundTruth, batchSize),  steps_per_epoch=dataTrain.shape[0]/batchSize, epochs=epochs, verbose=verbose)\n",
    "    return history\n",
    "\n",
    "def evaluate(batchSize = 10, path = testPath+\"test_feed.csv\"):\n",
    "    dataTrain, groundTruth = transformInput(path)\n",
    "    result = model.evaluate_generator(dataGenerator(dataTrain, groundTruth, batchSize),  steps=dataTrain.shape[0]/batchSize, verbose=0)\n",
    "    return result\n",
    "\n",
    "def predict(data):\n",
    "    return model.predict(data)\n",
    "\n",
    "def save(model, history, res):\n",
    "    def getLastIndex():\n",
    "        folder = [f for f in glob.iglob(classifierPath+\"run_*\")]\n",
    "        return len(folder)\n",
    "    processID = getLastIndex()+1\n",
    "    workingDir = classifierPath+\"run_\"+str(processID)+\"/\"\n",
    "    os.mkdir(workingDir)\n",
    "\n",
    "    plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    metadata = {\n",
    "        \"process_id\":processID,\n",
    "        \"model_param\":history.params,\n",
    "        \"train_result\": history.history,\n",
    "        \"evaluation_result\":{\n",
    "            \"metric_name\":model.metrics_names,\n",
    "            \"value\":res\n",
    "        }\n",
    "    }\n",
    "    with open(workingDir+\"summary.json\", \"w\") as fw : \n",
    "        json.dump(metadata, fw, indent=4)\n",
    "\n",
    "    with open(workingDir+\"model_configuration.json\", \"w\") as fw : \n",
    "        json.dump(model.get_config(), fw)\n",
    "\n",
    "    with open(workingDir+'architecture.txt','w+') as f:\n",
    "        with redirect_stdout(f):\n",
    "            model.summary()\n",
    "            \n",
    "    model.save_weights(workingDir+\"model_weight.h5\")\n",
    "    \n",
    "    plot_model(model, to_file= workingDir+'model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "def loadModel(processID):\n",
    "    path = classifierPath+\"run_\"+str(processID)\n",
    "    with open(path+\"/model_configuration.json\", 'r') as json_file:\n",
    "        json_file = json.load(json_file)\n",
    "    model = tf.keras.Sequential.from_config((json_file))\n",
    "    model.load_weights(path + \"/model_weight.h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_izUlyg2dsTm"
   },
   "source": [
    "**Tunning di marih**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EuwafCJVDMb"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Input((None, 1262)),\n",
    "  tf.keras.layers.LSTM(1262,activation='tanh', recurrent_activation='sigmoid',use_bias=True, return_sequences=True),\n",
    "  tf.keras.layers.LSTM(1262,activation='tanh', recurrent_activation='sigmoid',use_bias=True),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "model.build((None, None,1262))\n",
    "\n",
    "history = train(batchSize=10, epochs=10 , verbose=1)\n",
    "resp = evaluate()\n",
    "save(model, history, resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrqvdCesdxZa"
   },
   "source": [
    "**Visualisasi History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "civ4CAVEnDvR"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "summaryPaths = [p for p in Path(classifierPath).rglob('summary.json')]\n",
    "\n",
    "for summaryPath in summaryPaths:\n",
    "    with open(summaryPath,\"r\") as f:\n",
    "        summary = (json.load(f))\n",
    "        print(\"===========================xxxxxxxxxxx===============================\")\n",
    "        print(\"process id: \",summary[\"process_id\"])\n",
    "        print(\"parameter : \",summary[\"model_param\"])\n",
    "\n",
    "        summaryPath = str(summaryPath).split(\"/\")[:-1]\n",
    "        summaryPath = \"/\".join(summaryPath)+\"/architecture.txt\"\n",
    "        print(open(summaryPath, \"r\").read())\n",
    "\n",
    "        print(\"evaluation loss\" , summary[\"evaluation_result\"][\"value\"][0])\n",
    "        print(\"evaluation accuracy \",summary[\"evaluation_result\"][\"value\"][1])\n",
    "        print()\n",
    "        plt.plot(summary[\"train_result\"][\"loss\"])\n",
    "        plt.plot(summary[\"train_result\"][\"accuracy\"])\n",
    "        plt.title('model training accuracy loss')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['loss', 'accuracy'], loc='upper left')\n",
    "        plt.show()\n",
    "        print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMlyvKTrJfEx0yxGpCG2HCj",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "sentiment analysist .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
