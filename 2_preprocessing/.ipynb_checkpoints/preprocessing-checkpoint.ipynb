{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. parsing dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read each chats session folder path\n",
    "\"\"\" 1. list folder chatx DONE\n",
    "    2. list file dalam folder\n",
    "    \n",
    "    EXPECTED:\n",
    "    current dir + /parsed doc/ + chatx/ + sesix.txt    \n",
    "\"\"\"\n",
    "parsed_doc_path = '../1_parsing/parsed_doc/'\n",
    "chat_folders = [chat_folder for chat_folder in os.listdir(parsed_doc_path) if not chat_folder.startswith('.')]\n",
    "\n",
    "paths = []\n",
    "for chat_folder in chat_folders:\n",
    "    pattern = parsed_doc_path + chat_folder\n",
    "    paths.append(pattern)\n",
    "    \n",
    "sesi_paths = []\n",
    "for path in paths:\n",
    "    for roots, dirs, files in os.walk(path):\n",
    "        if \"checkpoints\" not in roots:\n",
    "            for file in files:\n",
    "                sesi_paths.append(roots + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make list of dataframe for each chats session\n",
    "sessions = list()\n",
    "for sesi_path in sesi_paths:\n",
    "    sesi = pd.read_csv(sesi_path, sep='|', names=['sender', 'messages', 'sentiment'])\n",
    "    sesi.drop('sender', axis=1, inplace=True) ## drop sender column (since its not necessary)\n",
    "    sessions.append(sesi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assalamualaikum Pak [DOSEN], mohon maaf mengga...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wa'alaikumsalam Wr. Wb. Baik, saya bersedia. I...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baik bapak, terimakasih. Mohon maaf sebelumnya...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages  sentiment\n",
       "0  Assalamualaikum Pak [DOSEN], mohon maaf mengga...        1.0\n",
       "1  Wa'alaikumsalam Wr. Wb. Baik, saya bersedia. I...        1.0\n",
       "2  Baik bapak, terimakasih. Mohon maaf sebelumnya...       -1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. lexical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lexical analysis - cleaning\n",
    "def cleaning(message):\n",
    "    \"\"\" This function will remove unnecessaries values in the messages i.e links, emails, and punctuations\n",
    "    \"\"\"\n",
    "    link_reg = r'(https?:\\/\\/[^\\s]+)|(www\\.[^\\s]+)|(meet\\.google\\.[^\\s]+)|(bit\\.ly[^\\s]+)'\n",
    "    email_reg = r'([a-zA-Z0-9\\.\\_\\-]+@+[a-zA-Z0-9.]+)'\n",
    "    punct_reg = r'[^a-zA-Z0-9\\[\\]]'\n",
    "    numb_reg = r'\\b[0-9]+\\b\\s*'\n",
    "    \n",
    "    message = re.sub(link_reg, '[LINK]', message) # -> link removed\n",
    "    message = re.sub(email_reg, '', message) # -> email removed\n",
    "    message = re.sub(punct_reg, ' ', message) # -> punctuation removed\n",
    "    message = re.sub(numb_reg, '', message) # -> numbers removed\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lexical analysis - lower, except <MHS>, <DOSEN>, <LINK>\n",
    "def lower(word_list):\n",
    "    new_word_list = list()\n",
    "    for word in word_list:\n",
    "        if word != '[MHS]' and word != '[DOSEN]' and word != '[LINK]':\n",
    "            word = word.lower()\n",
    "        new_word_list.append(word)\n",
    "    return new_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do lexical analysis\n",
    "for sesi in sessions:\n",
    "    for i, row in sesi.iterrows():\n",
    "        message = row['messages']\n",
    "        # do cleaning\n",
    "        message = cleaning(message)\n",
    "        # do strip\n",
    "        message = message.strip()\n",
    "        # do tokenization\n",
    "        message = re.split('\\s+', message)\n",
    "        # do lower case\n",
    "        message = lower(message)\n",
    "        \n",
    "        # replace current message value in df\n",
    "        sesi.at[i, 'messages'] = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[assalamualaikum, pak, [DOSEN], mohon, maaf, m...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[wa, alaikumsalam, wr, wb, baik, saya, bersedi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[baik, bapak, terimakasih, mohon, maaf, sebelu...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages  sentiment\n",
       "0  [assalamualaikum, pak, [DOSEN], mohon, maaf, m...        1.0\n",
       "1  [wa, alaikumsalam, wr, wb, baik, saya, bersedi...        1.0\n",
       "2  [baik, bapak, terimakasih, mohon, maaf, sebelu...       -1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. normalization (slang word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize function\n",
    "def normalize(slang_words, tokens):\n",
    "    \"\"\" This function will normalize the tokens,\n",
    "        it will turn the slang words or typos to its normal values.\n",
    "        NOTE: you could add the values into json files.\n",
    "    \"\"\"\n",
    "    new_tokens = list()\n",
    "    for token in tokens:\n",
    "        new_value = slang_words.get(token, token) #2nd parameter for default value if word's key not found\n",
    "        new_tokens.append(new_value)\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## open slang_words json\n",
    "f = open('slang_words.json', 'r')\n",
    "slang_words = json.load(f)\n",
    "f.close()\n",
    "\n",
    "## do normalize\n",
    "for sesi in sessions:\n",
    "    for i, row in sesi.iterrows():\n",
    "        tokens = row['messages']\n",
    "        # do normalize\n",
    "        new_tokens = normalize(slang_words, tokens)\n",
    "\n",
    "        #replace current message value in df\n",
    "        sesi.at[i, 'messages'] = new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[assalamualaikum, bapak, [DOSEN], mohon, maaf,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[wa, alaikumsalam, wr, wb, baik, saya, bersedi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[baik, bapak, terima kasih, mohon, maaf, sebel...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages  sentiment\n",
       "0  [assalamualaikum, bapak, [DOSEN], mohon, maaf,...        1.0\n",
       "1  [wa, alaikumsalam, wr, wb, baik, saya, bersedi...        1.0\n",
       "2  [baik, bapak, terima kasih, mohon, maaf, sebel...       -1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering function\n",
    "def filtering(stopword_list, tokens):\n",
    "    \"\"\" Filtering: removing stopwords from tokens.\n",
    "        In this project, we will use tala stopwords list.\n",
    "    \"\"\"\n",
    "    new_tokens = list()\n",
    "    for token in tokens:\n",
    "        if token not in stopword_list:\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## open stopword list txt\n",
    "f = open('tala_stopwords.txt', 'r')\n",
    "stopword_list = f.read()\n",
    "f.close()\n",
    "\n",
    "## do normalize\n",
    "for sesi in sessions:\n",
    "    for i, row in sesi.iterrows():\n",
    "        tokens = row['messages']\n",
    "        # do filtering\n",
    "        new_tokens = filtering(stopword_list, tokens)\n",
    "\n",
    "        #replace current message value in df\n",
    "        sesi.at[i, 'messages'] = new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[DOSEN], mohon, maaf, mengganggu, nama, [MHS]...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bersedia, topik, layak, posisinya, dimap, kaj...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[terima kasih, mohon, maaf, mekanisme, bimbing...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages  sentiment\n",
       "0  [[DOSEN], mohon, maaf, mengganggu, nama, [MHS]...        1.0\n",
       "1  [bersedia, topik, layak, posisinya, dimap, kaj...        1.0\n",
       "2  [terima kasih, mohon, maaf, mekanisme, bimbing...       -1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stemming function, except [DOSEN], [MHS], [LINK]\n",
    "def stemming(stemmer, tokens):\n",
    "    \"\"\" Stemming: returns words to its original form.\n",
    "        Since non-alphanumeric will be discarded by using StemmerFactory(),\n",
    "        This function will do stemming if the token values neither [dosen] nor [mhs].\n",
    "    \"\"\"\n",
    "    new_tokens = list()\n",
    "    for token in tokens:\n",
    "        if token != '[MHS]' and token != '[DOSEN]' and token != '[LINK]':\n",
    "            token = stemmer.stem(token)\n",
    "        new_tokens.append(token)\n",
    "    \n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create stemmer object\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "## do normalize\n",
    "for sesi in sessions:\n",
    "    for i, row in sesi.iterrows():\n",
    "        tokens = row['messages']\n",
    "        # do filtering\n",
    "        new_tokens = stemming(stemmer, tokens)\n",
    "\n",
    "        # replace current message value in df\n",
    "        sesi.at[i, 'messages'] = new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[DOSEN], mohon, maaf, ganggu, nama, [MHS], an...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[sedia, topik, layak, posisi, map, kaji, pustaka]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[terima kasih, mohon, maaf, mekanisme, bimbing...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages  sentiment\n",
       "0  [[DOSEN], mohon, maaf, ganggu, nama, [MHS], an...        1.0\n",
       "1  [sedia, topik, layak, posisi, map, kaji, pustaka]        1.0\n",
       "2  [terima kasih, mohon, maaf, mekanisme, bimbing...       -1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. add session index for each chat, also make array of df into single df, using pandas concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add index session column\n",
    "for i, sesi in enumerate(sessions):\n",
    "    index = [i for j in range(sesi.shape[0])]\n",
    "    sesi['session_index'] = index\n",
    "\n",
    "## concat all df into single df\n",
    "sessions_final = pd.concat(sessions, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. drop empty preprocessing result's message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 29, 53, 82, 116, 169, 196, 352, 388, 447, 451, 512, 569, 579, 625, 648, 756, 775, 783, 840, 873, 880, 982, 984, 1028, 1117, 1122, 1183, 1187, 1195, 1200, 1217, 1241, 1243, 1248, 1254, 1274, 1298, 1352, 1377, 1392, 1530, 1533, 1540]\n"
     ]
    }
   ],
   "source": [
    "empty_index = list()\n",
    "for i, row in sessions_final.iterrows():\n",
    "    if not row['messages']:\n",
    "        empty_index.append(i)\n",
    "\n",
    "print(empty_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_final = sessions_final.drop(sessions_final.index[empty_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no empty message!\n"
     ]
    }
   ],
   "source": [
    "## check again the empty index\n",
    "empty_index = list()\n",
    "for i, row in sessions_final.iterrows():\n",
    "    if not row['messages']:\n",
    "        empty_index.append(i)\n",
    "\n",
    "if not empty_index:\n",
    "    print('no empty message!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_final.to_csv('preprocessing_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
