{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 1. list folder chatx DONE\n",
    "    2. list file dalam folder\n",
    "    \n",
    "    EXPECTED:\n",
    "    current dir + /parsed doc/ + chatx/ + sesix.txt    \n",
    "\"\"\"\n",
    "folder = os.getcwd() + '/parsed doc/'\n",
    "chat_folders = [chat_folder for chat_folder in os.listdir(folder) if not chat_folder.startswith('.')]\n",
    "\n",
    "paths = []\n",
    "for i in range(len(chat_folders)):\n",
    "    pattern = str(folder) + chat_folders[i]\n",
    "    paths.append(pattern)\n",
    "    \n",
    "sesi_paths = []\n",
    "for i in range(len(paths)):\n",
    "    for roots, dirs, files in os.walk(paths[i]):\n",
    "        if \"checkpoints\" not in roots:\n",
    "            for file in files:\n",
    "                sesi_paths.append(roots + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats = [pd.read_csv(sesi, sep='|', names = ['sender', 'messages', 'sentiment']) for sesi in sesi_paths]\n",
    "\n",
    "messages = [chats[i].messages.values.tolist() for i in range(len(sesi_paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sender = chats['sender'].values.tolist()\n",
    "# messages = chats['messages'].values.tolist()\n",
    "# sentiment = chats['sentiment'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(msg):\n",
    "    \"\"\" This function will turns all sentences into token or word.\n",
    "        Each messages will saved in a list.\n",
    "    \"\"\"\n",
    "    token = [re.split('\\s+', msg[i]) for i, chat in enumerate(msg)]\n",
    "    return token\n",
    "\n",
    "token=[]\n",
    "for i in range(len(messages)):\n",
    "    token.append(tokenize(messages[i]))\n",
    "\n",
    "# token = [tokenize(messages[i]) for i in range(len(messages))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casefolding(msg):\n",
    "    \"\"\" This function will turn all the letters into lowercase. \"\"\"\n",
    "#     token_lower = [[token.lower() for token in i if token] for i in msg]\n",
    "    token_lower = [token.lower() for token in msg if token]\n",
    "    return token_lower\n",
    "\n",
    "token_lower = []\n",
    "for token_per_sesi in token:\n",
    "    for token_per_chat in token_per_sesi:\n",
    "        token_lower.append([casefolding(token_per_chat)])\n",
    "# token_lower = [casefolding(token) for i in range(len(messages))]\n",
    "# token_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(msg):\n",
    "    \"\"\" This function will remove unnecessaries values in the sentence i.e links, emails, and punctuations\n",
    "    \"\"\"\n",
    "    link = r'(https?:\\/\\/[^\\s]+)|(www\\.[^\\s]+)|(meet\\.google\\.[^\\s]+)|(bit\\.ly[^\\s]+)'\n",
    "    email = r'([a-zA-Z0-9\\.\\_\\-]+@+[a-zA-Z0-9.]+)'\n",
    "    punct = r'[^a-zA-Z0-9\\[\\]]'\n",
    "    \n",
    "    token_clean = []\n",
    "    for token in msg:\n",
    "        if '<doc>' in token:\n",
    "            token_clean.append(token)\n",
    "        else:  \n",
    "            # remove link\n",
    "            temp = re.sub(link, '', token)\n",
    "            # remove email\n",
    "            temp = re.sub(email, '', temp)\n",
    "            # remove punctuation\n",
    "            temp = re.sub(punct, ' ', temp)\n",
    "    #         temp = re.sub(r'\\-', ' ', temp)\n",
    "            # remove numbers\n",
    "            temp = re.sub(r'\\b[0-9]+\\b\\s*', '', temp)\n",
    "            token_clean.append(temp)\n",
    "    return token_clean\n",
    "\n",
    "# token_clean = cleaning(token_lower)\n",
    "token_clean = []\n",
    "for token_per_sesi in token_lower:\n",
    "    for token_per_chat in token_per_sesi:\n",
    "        token_clean.append([cleaning(token_per_chat)])\n",
    "# token_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(msg):\n",
    "    \"\"\" This function will normalize the tokens,\n",
    "        it will turn the slang words or typos to its normal values.\n",
    "        NOTE: you could add the values into json files.\n",
    "    \"\"\"\n",
    "    with open('slang_words.json', 'r') as f:\n",
    "        dict = json.load(f)\n",
    "        \n",
    "    normalized_chat = []\n",
    "    for token in msg:\n",
    "        slang_dict = {v:k for v, k in dict.items()}\n",
    "#         token_per_chat = []\n",
    "#         for token in i:\n",
    "        normal = slang_dict.get(token, token)\n",
    "#             token_per_chat.append(normal)\n",
    "        normalized_chat.append(normal)\n",
    "    return normalized_chat\n",
    "\n",
    "# token_normal = normalize(token_clean)\n",
    "token_normal = []\n",
    "for token_per_sesi in token_clean:\n",
    "    for token_per_chat in token_per_sesi:\n",
    "        token_normal.append([normalize(token_per_chat)])\n",
    "# token_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(msg):\n",
    "    \"\"\" Filtering: removing stopwords from tokens.\n",
    "        In this project, we will use tala stopwords list.\n",
    "    \"\"\"\n",
    "    with open('stopword_list_tala.txt', 'r') as tala:\n",
    "        stoplist = tala.read()\n",
    "    token_filtered = [token for token in msg if not token in stoplist]\n",
    "    return token_filtered\n",
    "\n",
    "# token_filtered = filtering(token_normal)\n",
    "token_filtered = []\n",
    "for token_per_sesi in token_normal:\n",
    "    for token_per_chat in token_per_sesi:\n",
    "        token_filtered.append([filtering(token_per_chat)])\n",
    "# token_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(msg):\n",
    "    \"\"\" Stemming: returns words to its original form.\n",
    "        Since non-alphanumeric will be discarded by using StemmerFactory(),\n",
    "        This function will do stemming if the token values neither [dosen] nor [mhs].\n",
    "    \"\"\"\n",
    "    stemmer = StemmerFactory().create_stemmer()\n",
    "    token_stemmed = []\n",
    "    for token in msg:\n",
    "        if token.startswith('['): # for [mhs] and [dosen]\n",
    "            token_stemmed.append(token)\n",
    "        else: token_stemmed.append(stemmer.stem(token))\n",
    "#     token_stemmed.append(token)\n",
    "    return token_stemmed\n",
    "\n",
    "# token_stemmed = stemming(token_filtered)\n",
    "token_stemmed = []\n",
    "for token_per_sesi in token_filtered:\n",
    "    for token_per_chat in token_per_sesi:\n",
    "        token_stemmed.append([stemming(token_per_chat)])\n",
    "# token_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setToken = [set(token) for token in token_stemmed]\n",
    "# setToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in setToken:\n",
    "#     print(len(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
