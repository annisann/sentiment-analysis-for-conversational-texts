{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 1. list folder chatx DONE\n",
    "    2. list file dalam folder\n",
    "    \n",
    "    EXPECTED:\n",
    "    current dir + /parsed doc/ + chatx/ + sesix.txt    \n",
    "\"\"\"\n",
    "folder = os.getcwd() + '/parsed doc/'\n",
    "chat_folders = [chat_folder for chat_folder in os.listdir(folder) if not chat_folder.startswith('.')]\n",
    "\n",
    "paths = []\n",
    "for i in range(len(chat_folders)):\n",
    "    pattern = str(folder) + chat_folders[i]\n",
    "    paths.append(pattern)\n",
    "    \n",
    "sesi_paths = []\n",
    "for i in range(len(paths)):\n",
    "    for roots, dirs, files in os.walk(paths[i]):\n",
    "        if \"checkpoints\" not in roots:\n",
    "            for file in files:\n",
    "                sesi_paths.append(roots + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Baik [MHS],  Silahkan dikirimkan dokumen bimbingan minimal per 1-2 mingguan utk pra Proposal/P0/P1/P2 sampai *Semhas/*Sidang.. Tlg cantumkan secara detail di dalam content email: + lampirkan draft file dok skripsi dlm .doc/.docx + ppt P0/P1/P2/Semhas/Sidang (jika mau P0/P1/P2/Semhas/Sidang), contoh ppt sdh ada di link grup kelas, tinggal diganti isinya atau dgn template lain + beberapa screenshoot hasil running program (jika sdh ada) + deskripsi point\" revisi sebelumnya (jika sdh ada)  + deskripsi point\" progress yg baru selesai dikerjakan (jika sdh ada) + hal\" yg ditanyakan (jika ada) + dll  *)utk Semhas/Sidang ditetapkan semuanya dilakukan online/daring dengan google Meet/Zoom/ lainnya.  Kirimkan ke [DOSEN]cs@ub.ac.id  #cegah Covid-19 > Ikhtiyar yg optimal (seperti physical & social distance (sebaiknya hindari/meminimalkan dulu berkumpul dgn mhs/org lain dan hindari kerja kelompok), stay at home/kos (batasi keluar, kecuali jika memang sangat penting dan urgent dgn pakai masker dan lainnya), jgn mudik dulu, gerakan hidup sehat dan bersih, dll) ==> lahir > Do\\'a dan Tawakal ==> batin  Semoga sukses untuk skripsi dan semuanya dan semoga kita semua terhindar dari bahaya/penyakit/wabah apapun. Aamiin.   Mohon maaf jika ada salah dan khilaf, Tetap selalu jaga Sikap Sopan, Santun dan Menyejukkan. Terimakasih.  [DOSEN] ', 'Baik bapak, terimakasih banyak informasinya ']\n"
     ]
    }
   ],
   "source": [
    "chats = [pd.read_csv(sesi, sep='|', names = ['sender', 'messages', 'sentiment']) for sesi in sesi_paths]\n",
    "\n",
    "# messages = [chats[i].messages.values.tolist() for i in range(len(sesi_paths))]\n",
    "\n",
    "messages=[]\n",
    "for i in range(len(sesi_paths)):\n",
    "    chat = chats[i].messages.values.tolist()\n",
    "    messages.append(chat)\n",
    "print(messages[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sender = chats['sender'].values.tolist()\n",
    "# messages = chats['messages'].values.tolist()\n",
    "# sentiment = chats['sentiment'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(messages):\n",
    "    \"\"\" This function will turns all sentences into token or word.\n",
    "        Each messages will saved in a list.\n",
    "    \"\"\"\n",
    "    token_per_sesi = []\n",
    "    for sesi in messages:\n",
    "        token_per_chat = []\n",
    "        for chat in sesi:\n",
    "            token_per_chat.append(re.split('\\s+',chat))\n",
    "        token_per_sesi.append(token_per_chat)\n",
    "    return token_per_sesi\n",
    "\n",
    "token = tokenize(messages)\n",
    "# token = [tokenize(messages[i]) for i in range(len(messages))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def casefolding(token):\n",
    "    \"\"\" This function will turn all the letters into lowercase. \"\"\"\n",
    "    token_per_sesi = []\n",
    "    for sesi in token:\n",
    "        token_per_chat = []\n",
    "        for chat in sesi:\n",
    "            token = []\n",
    "            for word in chat:\n",
    "                token.append(word.lower())\n",
    "            token_per_chat.append(token)\n",
    "        token_per_sesi.append(token_per_chat)\n",
    "    return token_per_sesi\n",
    "\n",
    "token_lower = casefolding(token)\n",
    "len(token_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(token):\n",
    "    \"\"\" This function will remove unnecessaries values in the sentence i.e links, emails, and punctuations\n",
    "    \"\"\"\n",
    "    link = r'(https?:\\/\\/[^\\s]+)|(www\\.[^\\s]+)|(meet\\.google\\.[^\\s]+)|(bit\\.ly[^\\s]+)'\n",
    "    email = r'([a-zA-Z0-9\\.\\_\\-]+@+[a-zA-Z0-9.]+)'\n",
    "    punct = r'[^a-zA-Z0-9\\[\\]]'\n",
    "    \n",
    "    token_per_sesi = []\n",
    "    for sesi in token:\n",
    "        token_per_chat = []\n",
    "        for chat in sesi:\n",
    "            token = []\n",
    "            for word in chat:\n",
    "                if '<doc>' in word:\n",
    "                    token.append(word)\n",
    "                else:\n",
    "                    temp = re.sub(link, '', word)\n",
    "                    # remove email\n",
    "                    temp = re.sub(email, '', temp)\n",
    "                    # remove punctuation\n",
    "                    temp = re.sub(punct, ' ', temp)\n",
    "            #         temp = re.sub(r'\\-', ' ', temp)\n",
    "                    # remove numbers\n",
    "                    temp = re.sub(r'\\b[0-9]+\\b\\s*', '', temp)\n",
    "                token.append(temp)\n",
    "            token_per_chat.append(token)\n",
    "        token_per_sesi.append(token_per_chat)\n",
    "    return token_per_sesi\n",
    "\n",
    "token_clean = cleaning(token_lower)\n",
    "token_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(token):\n",
    "    \"\"\" This function will normalize the tokens,\n",
    "        it will turn the slang words or typos to its normal values.\n",
    "        NOTE: you could add the values into json files.\n",
    "    \"\"\"\n",
    "    with open('slang_words.json', 'r') as f:\n",
    "        dict = json.load(f)\n",
    "        \n",
    "    token_per_sesi = []\n",
    "    for sesi in token:\n",
    "        token_per_chat = []\n",
    "        for chat in sesi:\n",
    "            token = []\n",
    "            for word in chat:\n",
    "                slang_dict = {v:k for v, k in dict.items()}\n",
    "                normal = slang_dict.get(word, word)\n",
    "                token.append(normal)\n",
    "            token_per_chat.append(token)\n",
    "        token_per_sesi.append(token_per_chat)\n",
    "    return token_per_sesi\n",
    "\n",
    "token_normal = normalize(token_clean)\n",
    "len(token_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filtering(token):\n",
    "    \"\"\" Filtering: removing stopwords from tokens.\n",
    "        In this project, we will use tala stopwords list.\n",
    "    \"\"\"\n",
    "    with open('stopword_list_tala.txt', 'r') as tala:\n",
    "        stoplist = tala.read()\n",
    "        \n",
    "    token_per_sesi = []\n",
    "    for sesi in token:\n",
    "        token_per_chat = []\n",
    "        for chat in sesi:\n",
    "            token = []\n",
    "            for word in chat:\n",
    "                if not word in stoplist:\n",
    "                    token.append(word)\n",
    "            token_per_chat.append(token)\n",
    "        token_per_sesi.append(token_per_chat)\n",
    "    return token_per_sesi\n",
    "\n",
    "token_filtered = filtering(token_normal)\n",
    "len(token_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(token):\n",
    "    \"\"\" Stemming: returns words to its original form.\n",
    "        Since non-alphanumeric will be discarded by using StemmerFactory(),\n",
    "        This function will do stemming if the token values neither [dosen] nor [mhs].\n",
    "    \"\"\"\n",
    "    stemmer = StemmerFactory().create_stemmer()\n",
    "    \n",
    "    token_per_sesi = []\n",
    "    for sesi in token:\n",
    "        token_per_chat = []\n",
    "        for chat in sesi:\n",
    "            token = []\n",
    "            for word in chat:\n",
    "                if word.startswith('['): #for [mhs] and [dosen]\n",
    "                    token.append(word)\n",
    "            else:\n",
    "                token_per_chat.append(stemmer.stem(word))\n",
    "        token_per_sesi.append(token_per_chat)\n",
    "    return token_per_sesi\n",
    "\n",
    "token_stemmed = stemming(token_filtered)\n",
    "len(token_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setToken = [set(token) for token in token_stemmed]\n",
    "# setToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in setToken:\n",
    "#     print(len(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
